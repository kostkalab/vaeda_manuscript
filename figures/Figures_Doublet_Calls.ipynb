{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-address",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import *\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join, isdir\n",
    "import pathlib as pl\n",
    "\n",
    "from plot_results import get_dbl_metrics\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, matthews_corrcoef\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2629c7a-5565-4854-915b-46e7a3e5ab70",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '../results_manuscript/doublet_calls/'\n",
    "\n",
    "path = '../results_benchmark/'\n",
    "files = [f.split('_') for f in listdir(path) if (isfile(join(path, f)) & (f[-3:]=='csv') and (f.split('_')[1]!='lib-sze'))]\n",
    "files = [f.split('_') for f in listdir(path) if (isfile(join(path, f)) & (f[-3:]=='csv') )]\n",
    "files = np.array(files)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab62699a-4e7e-43cf-9070-ac3cd0a157d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_names = np.sort(np.unique(files[:,0]))\n",
    "methods = np.unique(files[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3166a7ed-b92e-40a3-afe7-2df533eb97f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c49022-caa4-422b-a60a-67fa23dfd580",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['method', 'data_name', 'actual_frac', 'expected_frac', 'estimated_frac', \n",
    "        'f1', 'mcc', 'precision', 'recall', 'accuracy', 'doublets removed ÷ total doublets',\n",
    "        'singlets removed ÷ total removed']\n",
    "num = len(data_names)*len(methods)\n",
    "\n",
    "tmp1 = np.zeros((num, len(cols)))\n",
    "df_res = pd.DataFrame(tmp1, columns=cols)\n",
    "\n",
    "tmp2 = np.zeros((num, len(cols)))\n",
    "df_exp = pd.DataFrame(tmp2, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66381488-0b64-419b-a655-8043650c3d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "for i, file in enumerate(files):\n",
    "    res = pd.read_csv(path + '_'.join(file))\n",
    "    scores = np.array(res['doublet_scores'])\n",
    "    calls = np.array(res['predicted_doublets'])\n",
    "    if not (isinstance(calls[0], np.int64)):\n",
    "        if (isinstance(calls[0], np.str)):\n",
    "            #fix Doublet Singlet annotation\n",
    "            new_calls = np.zeros(len(calls))\n",
    "            new_calls[((calls=='Doublet') | (calls=='doublet'))] = 1\n",
    "            calls = new_calls\n",
    "        elif (isinstance(calls[0], np.bool_)):\n",
    "            #fix boolean annotation\n",
    "            new_calls = np.zeros(len(calls))\n",
    "            new_calls[calls] = 1\n",
    "            calls = new_calls\n",
    "        elif (isinstance(calls[0], np.float)):\n",
    "            new_calls = calls.astype(int)\n",
    "            calls = new_calls\n",
    "        else:\n",
    "            print('UNKNOWN DATATYPE')\n",
    "            print(file)\n",
    "            print(100/0)\n",
    "                \n",
    "    ano_path  = '../data/mtx_files/' + file[0] + '_anno.csv'\n",
    "\n",
    "    #- READ IN BARCODE ANNOTATIONS\n",
    "    ano = pd.read_csv(ano_path)\n",
    "    true = pd.factorize(ano.x)[0]\n",
    "    labels = ano.x\n",
    "    if (labels[0]=='doublet'):\n",
    "        tmp = true + 3\n",
    "        tmp[tmp==3] = 1\n",
    "        tmp[tmp==4] = 0\n",
    "        true = tmp\n",
    "    \n",
    "    \n",
    "    total = len(calls)\n",
    "    actual = np.sum(true) / total\n",
    "    estimated = np.sum(calls) / total\n",
    "\n",
    "    df_res['method'].iloc[i] = file[1]\n",
    "    df_res['data_name'].iloc[i] = file[0]\n",
    "    \n",
    "    df_res['actual_frac'].iloc[i] = actual\n",
    "    df_res['expected_frac'].iloc[i] = (total)*(10**(-5))\n",
    "    df_res['estimated_frac'].iloc[i] = estimated\n",
    "\n",
    "    df_res['f1'].iloc[i] = (f1_score(true, calls, average=\"binary\"))#weighted\n",
    "    df_res['mcc'].iloc[i] = (matthews_corrcoef(true, calls))    \n",
    "    df_res['precision'].iloc[i] = (precision_score(true, calls, average=\"binary\"))\n",
    "    df_res['recall'].iloc[i] = (recall_score(true, calls, average=\"binary\"))\n",
    "    df_res['accuracy'].iloc[i] = (accuracy_score(true, calls))\n",
    "\n",
    "    removed = np.sum(calls[true==1])\n",
    "    df_res['doublets removed ÷ total doublets'].iloc[i] = removed / np.sum(true) \n",
    "    df_res['singlets removed ÷ total removed'].iloc[i] = np.sum(calls[true==0]) / np.sum(calls)\n",
    "    \n",
    "    \n",
    "    \n",
    "    exp_num = int(len(scores)**2/(10**5))\n",
    "    exp_calls = np.zeros(len(scores))\n",
    "    ind = np.argsort(scores)[-exp_num:]\n",
    "    exp_calls[ind] = 1\n",
    "    \n",
    "    df_exp['method'].iloc[i] = file[1]\n",
    "    df_exp['data_name'].iloc[i] = file[0]\n",
    "    \n",
    "    df_exp['actual_frac'].iloc[i] = actual\n",
    "    df_exp['expected_frac'].iloc[i] = (total)*(10**(-5))\n",
    "    df_exp['estimated_frac'].iloc[i] = estimated\n",
    "\n",
    "    df_exp['f1'].iloc[i] = (f1_score(true, exp_calls, average=\"binary\"))#weighted\n",
    "    df_exp['mcc'].iloc[i] = (matthews_corrcoef(true, exp_calls))    \n",
    "    df_exp['precision'].iloc[i] = (precision_score(true, exp_calls, average=\"binary\"))\n",
    "    df_exp['recall'].iloc[i] = (recall_score(true, exp_calls, average=\"binary\"))\n",
    "    df_exp['accuracy'].iloc[i] = (accuracy_score(true, exp_calls))\n",
    "\n",
    "    removed = np.sum(exp_calls[true==1])\n",
    "    df_exp['doublets removed ÷ total doublets'].iloc[i] = removed / np.sum(true) \n",
    "    df_exp['singlets removed ÷ total removed'].iloc[i] = np.sum(exp_calls[true==0]) / np.sum(exp_calls)\n",
    "    \n",
    "df_res.loc[df_res.method=='lib-sze',:] = df_exp.loc[df_res.method=='lib-sze',:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5cd7a8-3ce5-4b3e-9fdf-9d9ac45a7083",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WIDTH = 6.726"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79c3b11-42b7-454e-b52d-717ba8e093e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"image.cmap\"] = \"Dark2\"\n",
    "plt.rcParams['axes.prop_cycle'] = plt.cycler(color=plt.cm.Dark2.colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa5e231-fae5-4767-882a-271b3fa0ae94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corrs=[]\n",
    "for method in methods:\n",
    "    df = df_res.loc[df_res.method==method,:]\n",
    "    fig, ax = plt.subplots(figsize=(MAX_WIDTH,3), dpi=300)\n",
    "\n",
    "    plt.plot([0,0.4], [0,0.4], c='black')\n",
    "    plt.scatter(df.actual_frac, df.estimated_frac, alpha=0.8)\n",
    "    \n",
    "    corr = np.round(np.corrcoef(df.actual_frac, df.estimated_frac)[0,1], 3)\n",
    "    corrs.append(corr)\n",
    "    \n",
    "    plt.title('Doublet Fraction (R: ' + str(corr) + ')')\n",
    "    plt.xlabel('actual doublet fraction')\n",
    "    plt.ylabel('estimated doublet fraction')\n",
    "    #plt.legend(bbox_to_anchor=(1,1), loc=\"upper left\")\n",
    "\n",
    "    #ax = plt.axes()\n",
    "    ax.set_facecolor(\"white\")\n",
    "    plt.grid(False)\n",
    "    \n",
    "    #make square\n",
    "    x0,x1 = ax.get_xlim()\n",
    "    y0,y1 = ax.get_ylim()\n",
    "    ax.set_aspect(abs(x1-x0)/abs(y1-y0))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path + method + '_doublet_frac_scatter.png', dpi = 300)\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a94fa2-db06-4d15-a203-4f643e098631",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_res.loc[df_res.method=='vaeda',:]\n",
    "fig, ax = plt.subplots(figsize=(MAX_WIDTH,3), dpi=300)\n",
    "\n",
    "plt.plot([0,0.4], [0,0.4], c='black')\n",
    "plt.scatter(df.actual_frac, df.expected_frac, alpha=0.8)\n",
    "\n",
    "corr = np.round(np.corrcoef(df.actual_frac, df.expected_frac)[0,1], 3)\n",
    "corrs.append(corr)\n",
    "\n",
    "plt.title('Doublet Fraction (R: ' + str(corr) + ')')\n",
    "plt.xlabel('actual doublet fraction')\n",
    "plt.ylabel('expected doublet fraction')\n",
    "\n",
    "ax.set_facecolor(\"white\")\n",
    "plt.grid(False)\n",
    "\n",
    "#make square\n",
    "x0,x1 = ax.get_xlim()\n",
    "y0,y1 = ax.get_ylim()\n",
    "ax.set_aspect(abs(x1-x0)/abs(y1-y0))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_path + 'EXPECTED_doublet_frac_scatter.png', dpi = 300)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5600da8c-d2ae-44cc-aa07-5edfee83d659",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = df_res.groupby(['method']).agg('mean').sort_values(by='f1', ascending=False).index.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35ef1f5-cb13-4b32-a633-7b0bedffdcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc11540-f49f-4683-ac49-3eeff8b94144",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = ((df_res.groupby(['method']).agg('mean').sort_values(by='f1', ascending=False).loc[:,['f1', 'mcc', 'precision', 'recall', 'accuracy']])).round(3)\n",
    "table.to_csv(save_path + 'table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2109af-aa10-4040-a0ba-c7c8573908f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ca27fd-59a8-46ff-bf56-d14c7a9fab39",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = ((df_res.groupby(['method']).agg('mean').sort_values(by='f1', ascending=False).loc[methods,['f1', 'mcc', 'precision', 'recall', 'accuracy']]))\n",
    "d2 = df_exp.groupby(['method']).agg('mean').sort_values(by='f1', ascending=False).loc[methods,['f1', 'mcc', 'precision', 'recall', 'accuracy']]\n",
    "d2.round(3).to_csv(save_path + 'table_exp.csv')\n",
    "d1.subtract(d2).round(3).to_csv(save_path + 'table_caller-exp.csv') #if doub caller better than exp, possitive\n",
    "d1.subtract(d2).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf75190-4f96-40c0-86a6-de9eaa792be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_heatmap(hm, title = 'Heatmap'):\n",
    "    fig, ax = plt.subplots(1, 1, dpi=300)\n",
    "    cbar_kws = {\"orientation\":\"horizontal\", \n",
    "                \"shrink\":0.7,\n",
    "                'extendfrac':0.1, \n",
    "                \"drawedges\":False,\n",
    "               }\n",
    "    \n",
    "    hm_norm = (hm.subtract(hm.min(axis=1), axis=0)).div((hm.max(axis=1) - hm.min(axis=1)), axis=0) * 100\n",
    "    \n",
    "    ax=sns.heatmap(hm_norm, square=False, ax=ax, cbar=True, annot=hm, cmap=\"YlOrRd\",  fmt='g')#, cbar_kws=cbar_kws) #YlOrRd\n",
    "    ax.set_yticklabels(ax.get_yticklabels(),rotation=45)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(),rotation=90)\n",
    "    ax.set_title(title, fontsize=18)\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fb13ae-979a-45b2-a9e5-f7a3bc2ea41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1 = np.zeros((len(data_names), len(methods)))\n",
    "hm = pd.DataFrame(tmp1, index=data_names, columns=methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-infrared",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cols[5:-2]:\n",
    "    df = df_res\n",
    "    \n",
    "    df_means = df.groupby(['method'])[col].agg('mean').reset_index().sort_values([col], ascending=False)\n",
    "    pr_methods = np.array(df_means['method'])\n",
    "\n",
    "    sns.set(rc={\"figure.figsize\":(MAX_WIDTH, 4)})\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    df_var = df.groupby(['method'])[col].agg('var').reset_index()\n",
    "    df_var.index = df_var.method\n",
    "    df_var = df_var.loc[pr_methods,:]\n",
    "\n",
    "    plt.errorbar(df_means.method, df_means[col], np.sqrt(df_var[col]),\n",
    "                fmt='none', ecolor='black', capsize=5)\n",
    "    \n",
    "    ax = sns.violinplot(data=df, x='method', y=col, palette=\"Set2\", linewidth=0, dodge=True, order=pr_methods)\n",
    "    plt.setp(ax.collections, alpha=.3)\n",
    "\n",
    "    ax = sns.swarmplot(data=df, x='method', y=col, palette=\"Set2\", dodge=True, size=3, order=pr_methods)\n",
    "    \n",
    "    sns.swarmplot(data=df_means, x='method', y=col, marker='o', palette=\"Set2\", size=3, linewidth=1, edgecolor='black', order=pr_methods)\n",
    "    \n",
    "    #plt.legend(bbox_to_anchor=(1,1), loc=\"upper left\")\n",
    "    plt.xticks(rotation=45, horizontalalignment='right')\n",
    "    plt.title(col)\n",
    "    plt.xlabel('')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path + col + '_violin.png', dpi=300)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ec6acf-71d5-4b5c-b351-7860c29e1acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ranksums, wilcoxon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-colony",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cols[5:-2]:\n",
    "    print(col)\n",
    "    torny = pd.DataFrame(np.zeros((len(methods), len(methods))), index=methods, columns=methods)\n",
    "    df = df_res.loc[:,['method', 'data_name', col]]\n",
    "    df.index = df['data_name']\n",
    "    tmp = np.full((len(methods), len(methods)), 'ns')\n",
    "    sig = pd.DataFrame(tmp, index=methods, columns=methods)\n",
    "    for i in range(len(methods)):\n",
    "        #print('--', method, '--')\n",
    "        m1 = methods[i]\n",
    "        for j in range(i+1,len(methods)):\n",
    "            print\n",
    "            m2 = methods[j]\n",
    "            #print(m1, m2)\n",
    "            d1 = df.loc[df.method==m1,col]\n",
    "            d2 = df.loc[df.method==m2,col]\n",
    "            \n",
    "            d1 = d1.loc[data_names]\n",
    "            d2 = d2.loc[data_names]\n",
    "            \n",
    "            w = wilcoxon(d1, d2)\n",
    "            diff = (d1-d2).sum()\n",
    "\n",
    "            if((diff<0) and (w[1]<0.05)):\n",
    "                #loss for m1\n",
    "                #print(m1, m2, 'loss')\n",
    "                torny.loc[m1,m2] = m2\n",
    "                torny.loc[m2,m1] = m2\n",
    "            elif((diff>0) and (w[1]<0.05)):\n",
    "                #win for m1\n",
    "                #print(m1, m2, 'win')\n",
    "                torny.loc[m1,m2] = m1\n",
    "                torny.loc[m2,m1] = m1\n",
    "            else:\n",
    "                #tie\n",
    "                #print(m1, m2, 'tie')\n",
    "                torny.loc[m1,m2] = 'tie'\n",
    "                torny.loc[m2,m1] = 'tie'\n",
    "\n",
    "    print(torny)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563d1cbf-d89a-4dd0-a9f5-04ba14185aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "meths = ['vaeda', 'scDblFinder', 'DoubletFinder']\n",
    "for col in cols[5:-2]:\n",
    "    print(col)\n",
    "    torny = pd.DataFrame(np.zeros((len(meths), len(meths))), index=meths, columns=meths)\n",
    "    df = df_res.loc[:,['method', 'data_name', col]]\n",
    "    df.index = df['data_name']\n",
    "    tmp = np.full((len(meths), len(meths)), 'ns')\n",
    "    sig = pd.DataFrame(tmp, index=meths, columns=meths)\n",
    "    for i in range(len(meths)):\n",
    "        #print('--', meths, '--')\n",
    "        m1 = meths[i]\n",
    "        for j in range(i+1,len(meths)):\n",
    "            print\n",
    "            m2 = meths[j]\n",
    "            #print(m1, m2)\n",
    "            \n",
    "            d1 = df.loc[df.method==m1,col]\n",
    "            d2 = df.loc[df.method==m2,col]\n",
    "            \n",
    "            d1 = d1.loc[data_names]\n",
    "            d2 = d2.loc[data_names]\n",
    "            \n",
    "            w = wilcoxon(d1, d2)\n",
    "            diff = (d1-d2).sum()\n",
    "            \n",
    "            r = ranksums(d1, d2)\n",
    "            \n",
    "            print('m1', m1)\n",
    "            print('d1', d1.sum()/16)\n",
    "            print('m2', m2)\n",
    "            print('d2', d2.sum()/16)\n",
    "            print('diff', diff)\n",
    "            print('p', w[1])\n",
    "            print('p2', r[1])\n",
    "            \n",
    "            if((diff<0) and (w[1]<0.05)):\n",
    "                #loss for m1\n",
    "                #print(m1, m2, 'loss')\n",
    "                torny.loc[m1,m2] = m2\n",
    "                torny.loc[m2,m1] = m2\n",
    "            elif((diff>0) and (w[1]<0.05)):\n",
    "                #win for m1\n",
    "                #print(m1, m2, 'win')\n",
    "                torny.loc[m1,m2] = m1\n",
    "                torny.loc[m2,m1] = m1\n",
    "            else:\n",
    "                #tie\n",
    "                #print(m1, m2, 'tie')\n",
    "                torny.loc[m1,m2] = 'tie'\n",
    "                torny.loc[m2,m1] = 'tie'\n",
    "\n",
    "    print(torny)\n",
    "    #torny.to_csv(save_path + 'lookup_table_frac' + str(frac) + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dff19b3-fcb6-4ece-9f22-6b058e46c0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = files[files[:,1]=='vaeda',:]\n",
    "fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda1eab8-6306-4018-94cc-fca27494197f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_norm(x, mean, sd):\n",
    "    t1 = -np.log(sd*np.sqrt(2*math.pi))\n",
    "    t2 = (-.5)*((x-mean)/sd)**2\n",
    "    return t1+t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9aa034-9672-4099-8d25-741fed34253d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "threshes=[]\n",
    "\n",
    "plt.rcParams[\"image.cmap\"] = \"Dark2\"\n",
    "plt.rcParams['axes.prop_cycle'] = plt.cycler(color=plt.cm.Dark2.colors)\n",
    "\n",
    "for i, file in enumerate(fs):\n",
    "    \n",
    "    pth = '../results_PU/final_vaeda_result/VAEDA_new_calls/' + file[0] + '_scores_on_sim.npy'\n",
    "    scores_on_sim = np.load(pth)\n",
    "\n",
    "    res = pd.read_csv(path + '_'.join(file))\n",
    "    scores = np.array(res['doublet_scores'])\n",
    "    \n",
    "    maximum = np.max([np.max(scores), np.max(scores_on_sim)])\n",
    "    minimum = np.min([np.min(scores), np.min(scores_on_sim)])\n",
    "    \n",
    "    thresholds = np.arange(minimum,maximum,0.001)\n",
    "    \n",
    "    n = len(scores)\n",
    "    dbr = n/10**5\n",
    "    dbl_expected = n*dbr\n",
    "    dbr_sd = np.sqrt(n*dbr*(1-dbr))\n",
    "    \n",
    "    FNR = []\n",
    "    FPR = []\n",
    "    ll_doub=[]\n",
    "    \n",
    "    d_t = np.sum(scores>=thresholds[-1])\n",
    "    norm_factor = (log_norm(d_t, dbl_expected, dbr_sd))\n",
    "    \n",
    "    for thresh in thresholds:\n",
    "        \n",
    "        d_t = np.sum(scores>=thresh)\n",
    "        \n",
    "        FNR.append((np.sum(scores_on_sim<thresh)/len(scores_on_sim)))\n",
    "        FPR.append((d_t/len(scores)))  \n",
    "        ll_doub.append((log_norm(d_t, dbl_expected, dbr_sd)/norm_factor))\n",
    "        \n",
    "            \n",
    "    cost = np.array(FNR) + np.array(FPR) + np.array(ll_doub)**2\n",
    "    \n",
    "    t = thresholds[np.argmin(cost)]\n",
    "    clls = scores > t\n",
    "    \n",
    "    threshes.append(t)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(MAX_WIDTH,3), dpi=300)\n",
    "    \n",
    "    plt.plot(thresholds, FNR, linewidth=2, label='FNR')\n",
    "    plt.plot(thresholds, FPR, linewidth=2, label='FPR')\n",
    "    plt.plot(thresholds, np.array(ll_doub)**2, linewidth=2, label='\\u03B1LL\\u00b2')\n",
    "    plt.plot(thresholds, cost, linewidth=2, label='cost')\n",
    "\n",
    "    plt.plot([t,t], [0,1], c='black', alpha=0.7, linestyle='dotted', label='threshold')\n",
    "    \n",
    "    #ax = plt.axes()\n",
    "    ax.set_facecolor(\"white\")\n",
    "    plt.grid(False)\n",
    "    \n",
    "    #make square\n",
    "    plt.ylim(0,1)\n",
    "    x0,x1 = ax.get_xlim()\n",
    "    y0,y1 = ax.get_ylim()\n",
    "    ax.set_aspect(abs(x1-x0)/abs(y1-y0))\n",
    "    \n",
    "    #plt.legend()\n",
    "    plt.legend(bbox_to_anchor=(1,1), loc=\"upper left\")\n",
    "    plt.xlabel('thresholds')\n",
    "    plt.ylabel('cost')\n",
    "\n",
    "    plt.title(file[0])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path + file[0] + '_cost.png', dpi = 300)\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a49053f-0edf-4e2f-b116-2af96c86915b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "threshes=[]\n",
    "\n",
    "plt.rcParams[\"image.cmap\"] = \"Dark2\"\n",
    "plt.rcParams['axes.prop_cycle'] = plt.cycler(color=plt.cm.Dark2.colors)\n",
    "\n",
    "width = 15\n",
    "rows = 3\n",
    "cols = 6\n",
    "\n",
    "fig,axes = plt.subplots(figsize=(width,(width/cols)*rows),sharex=True, sharey=True,dpi=300)\n",
    "d = 1\n",
    "\n",
    "for i, file in enumerate(fs):\n",
    "    \n",
    "    pth = '../results_PU/final_vaeda_result/VAEDA_new_calls/' + file[0] + '_scores_on_sim.npy'\n",
    "    scores_on_sim = np.load(pth)\n",
    "\n",
    "    res = pd.read_csv(path + '_'.join(file))\n",
    "    scores = np.array(res['doublet_scores'])\n",
    "    \n",
    "    maximum = np.max([np.max(scores), np.max(scores_on_sim)])\n",
    "    minimum = np.min([np.min(scores), np.min(scores_on_sim)])\n",
    "    \n",
    "    thresholds = np.arange(minimum,maximum,0.001)\n",
    "    \n",
    "    n = len(scores)\n",
    "    dbr = n/10**5\n",
    "    dbl_expected = n*dbr\n",
    "    dbr_sd = np.sqrt(n*dbr*(1-dbr))\n",
    "    \n",
    "    FNR = []\n",
    "    FPR = []\n",
    "    ll_doub=[]\n",
    "    \n",
    "    d_t = np.sum(scores>=thresholds[-1])\n",
    "    norm_factor = (log_norm(d_t, dbl_expected, dbr_sd))\n",
    "    \n",
    "    for thresh in thresholds:\n",
    "        \n",
    "        d_t = np.sum(scores>=thresh)\n",
    "        \n",
    "        FNR.append((np.sum(scores_on_sim<thresh)/len(scores_on_sim)))\n",
    "        FPR.append((d_t/len(scores)))  \n",
    "        ll_doub.append((log_norm(d_t, dbl_expected, dbr_sd)/norm_factor))\n",
    "        \n",
    "            \n",
    "    cost = np.array(FNR) + np.array(FPR) + np.array(ll_doub)**2\n",
    "    \n",
    "    t = thresholds[np.argmin(cost)]\n",
    "    clls = scores > t\n",
    "    \n",
    "    threshes.append(t)\n",
    "    \n",
    "    ax = plt.subplot(rows, cols, d)\n",
    "    \n",
    "    plt.plot(thresholds, FNR, linewidth=2, label='FNR')\n",
    "    plt.plot(thresholds, FPR, linewidth=2, label='FPR')\n",
    "    plt.plot(thresholds, np.array(ll_doub)**2, linewidth=2, label='LL\\u00b2')\n",
    "    plt.plot(thresholds, cost, linewidth=2, label='cost')\n",
    "\n",
    "    plt.plot([t,t], [0,1], c='black', alpha=0.7, linestyle='dotted', label='threshold')\n",
    "    \n",
    "    #ax = plt.axes()\n",
    "    #ax.set_facecolor(\"white\")\n",
    "    plt.grid(False)\n",
    "    \n",
    "    #make square\n",
    "    plt.ylim(0,1)\n",
    "    x0 = thresholds.min()\n",
    "    x1 = thresholds.max()\n",
    "    y0,y1 =  (0,1)\n",
    "    ax.set_aspect(abs(x1-x0)/abs(y1-y0))\n",
    "    \n",
    "    #plt.legend()\n",
    "    #plt.legend(bbox_to_anchor=(1,1), loc=\"upper left\")\n",
    "    if (d>10):\n",
    "        plt.xlabel('thresholds')\n",
    "    if ((d==1)|(d==7)|(d==13)):\n",
    "        plt.ylabel('cost')\n",
    "    \n",
    "    d += 1\n",
    "\n",
    "    plt.title(file[0])\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig(save_path + 'ALL_cost.png', dpi = 300)\n",
    "    \n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29157588-71b2-4810-b911-507152a17f30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv",
   "language": "python",
   "name": "newenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
