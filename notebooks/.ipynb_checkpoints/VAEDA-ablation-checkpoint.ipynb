{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27406442-eead-4b88-816f-7b09f2b79d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as scs\n",
    "from scipy.stats import multinomial\n",
    "import pathlib as pl\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import mmread\n",
    "\n",
    "import umap as um\n",
    "\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import random\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_recall_curve, accuracy_score, average_precision_score\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from kneed import KneeLocator\n",
    "\n",
    "from numpy.random import seed\n",
    "\n",
    "import time\n",
    "from scipy.signal import savgol_filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae727a78-2d38-442d-b83d-82237e69aa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_results import get_dbl_metrics\n",
    "from vae import define_clust_vae, define_vae\n",
    "from PU import PU, epoch_PU2\n",
    "from PU import noPU\n",
    "from classifier import define_classifier\n",
    "from mk_doublets import sim_inflate, sim_avg, sim_sum\n",
    "from cluster import cluster, fast_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2357870-bc84-4155-a4ad-b2c3b1d47595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import anndata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ac18bf-cd0f-4a9f-a891-a8cf56ff3e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    sig = 1 / (1 + np.exp(-(12*x)+6))\n",
    "    return sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7440d706-7b00-4c5f-9930-5bb04ed4a707",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pca_comp=30, clust_weight=20000\n",
    "data_dir = 'sce_normalized_data_inflate'\n",
    "save = True\n",
    "use_old = True\n",
    "\n",
    "#VAE hyperparams\n",
    "eps  = 1000\n",
    "enc_sze = 5\n",
    "pat = 20\n",
    "LR=1e-3\n",
    "clust_weight = 20000\n",
    "\n",
    "#PU hyperparameters\n",
    "cls_eps = 250 \n",
    "stop_metric = 'ValAUC'\n",
    "puPat = 5\n",
    "puLR =1e-3\n",
    "pu_num_layers = 1\n",
    "k_mult = 2\n",
    "N = 1\n",
    "\n",
    "gene_thresh=.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf826d4-e3fa-4656-9955-ba2fda411449",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [42, 29503, 432809, 42, 132975, 9231996, 12883823, 9231996, 1234, 62938, 57203 ,109573, 23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69d24ae-6e0f-4c53-9d6c-807b2f5dc0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'sce_normalized_data_inflate'\n",
    "path = '../data/' + data_dir + '/'\n",
    "files = [f for f in listdir(path) if (isfile(join(path, f)) & (f[-18:-4] == 'real_logcounts'))]\n",
    "files = np.sort(files)\n",
    "#files = files[3:4]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdb3fe7-e68d-4d52-aae5-98629d9b27bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nohup jupyter nbconvert --to notebook --execute CLUST-VAEDA-ablation.ipynb > nohup_ablation_final.out &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693fb443-7ad9-477e-bda4-1457a1274d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_names = ['total', 'HVGs', 'scaling1', 'knn', 'downsample', 'scaling2', 'cluster', 'vae', 'epoch_selection', 'PU_loop']\n",
    "tmp1 = np.zeros((1, len(time_names)))\n",
    "time_df = pd.DataFrame(tmp1, index=['time'], columns=time_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e060519-659c-40d5-8170-b5c72a15cfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_reds = ['clust_vae', 'vae', 'pca']\n",
    "PUs      = ['PU', 'noPU']\n",
    "clsses   = ['knn', 'NN']\n",
    "feats    = ['knnfeat', 'nofeat']\n",
    "homos    = ['remove', 'keep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dad3f9-39cc-4dbd-a6d7-cb191d671407",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''dim_reds = ['clust_vae']\n",
    "PUs      = ['PU']\n",
    "clsses   = ['NN']\n",
    "feats    = ['knnfeat']\n",
    "homos    = ['remove']'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac96462-ebee-4875-93f7-76ae5bf01eef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for file in files:\n",
    "\n",
    "    data_name = file[:-19]\n",
    "    print(data_name) \n",
    "    \n",
    "    save_path = '../results_PU/ablation_analysis/'\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    print(save_path)\n",
    "        \n",
    "    real_path = '../data/mtx_files/' + data_name + '.mtx'\n",
    "    ano_path  = '../data/mtx_files/' + data_name + '_anno.csv'\n",
    "    \n",
    "    print('loading in real mtx')\n",
    "    dat_real = mmread(real_path)\n",
    "    Xr = scs.csr_matrix(dat_real).toarray().T\n",
    "    \n",
    "    #scvae-dbl-btch/data/sce_normalized_data_inflate/pdx-MULTI_real_logcounts.mtx\n",
    "    npz_sim_path  = save_path + '/' + data_name + '_sim_counts.npz'\n",
    "    sim_ind_path  = save_path + '/' + data_name + '_sim_ind.npy'\n",
    "\n",
    "    #- READ IN COUNTS\n",
    "    npz_sim  = pl.Path(npz_sim_path)    \n",
    "    \n",
    "    if (npz_sim.exists() & use_old):\n",
    "        print('loading in sim npz')\n",
    "        dat_sim = scs.load_npz(npz_sim)\n",
    "        sim_ind = np.load(sim_ind_path)\n",
    "        ind1 = sim_ind[0,:]\n",
    "        ind2 = sim_ind[1,:]\n",
    "        Xs = scs.csr_matrix(dat_sim).toarray()\n",
    "    else:\n",
    "        print('generating new sim npz')\n",
    "        Xs, ind1, ind2 = sim_inflate(Xr)\n",
    "    \n",
    "    dat = np.vstack([Xr,Xs])\n",
    "    Y0 = np.concatenate([np.zeros(Xr.shape[0]), np.ones(Xs.shape[0])])\n",
    "    \n",
    "    #- READ IN BARCODE ANNOTATIONS\n",
    "    ano = pd.read_csv(ano_path)\n",
    "    true = pd.factorize(ano.x)[0]\n",
    "    labels = ano.x\n",
    "    if (labels[0]=='doublet'):\n",
    "        tmp = true + 3\n",
    "        tmp[tmp==3] = 1\n",
    "        tmp[tmp==4] = 0\n",
    "        true = tmp\n",
    "    true0 = np.concatenate([true, np.full(Xs.shape[0],2)])\n",
    "    labels0 = np.concatenate([labels, np.full(Xs.shape[0],'simulated')])\n",
    "\n",
    "    #Filter genes\n",
    "    thresh = np.floor(dat.shape[0]) * gene_thresh\n",
    "    tmp    = np.sum((dat>0), axis=0)>thresh\n",
    "    dat = dat[:,tmp]\n",
    "    \n",
    "    #- HVGs - NEED TO DO SOMETHING ELSE!    \n",
    "    var = np.var(dat, axis=0)\n",
    "    np.random.seed(3900362577)\n",
    "    hvgs = np.argpartition(var, -2000)[-2000:]      \n",
    "    \n",
    "\n",
    "    #######################################################\n",
    "    ######################### KNN #########################\n",
    "    #######################################################\n",
    "    \n",
    "    TRUE = true\n",
    "    \n",
    "    for feat in feats:\n",
    "        \n",
    "        save_path = '../results_PU/ablation_analysis/'\n",
    "        \n",
    "        X1 = dat[:,hvgs]\n",
    "        Y1 = Y0\n",
    "        labels1 = labels0\n",
    "        true1 = true0\n",
    "        \n",
    "        print('dat shape:', dat.shape)\n",
    "        print('Y:', Y1.shape)\n",
    "    \n",
    "        if feat == 'knnfeat':\n",
    "            \n",
    "            knn_file = save_path + data_name + '_knn_feature.npy'\n",
    "            \n",
    "            if(pl.Path(knn_file).exists() & use_old):\n",
    "                print('loading old knn')\n",
    "                knn_feature1 = np.load(knn_file)\n",
    "            \n",
    "            else:\n",
    "                print('generating new knn')\n",
    "                #HYPERPARAMS\n",
    "                neighbors = int(np.sqrt(X1.shape[0]))\n",
    "                comp=30\n",
    "\n",
    "                #SCALING\n",
    "                temp_X = np.log2(X1+1)\n",
    "                np.random.seed(42)\n",
    "                scaler = StandardScaler().fit(temp_X.T)\n",
    "                np.random.seed(42)\n",
    "                temp_X = scaler.transform(temp_X.T).T\n",
    "\n",
    "                #KNN\n",
    "                np.random.seed(42)\n",
    "                pca = PCA(n_components=comp)\n",
    "                pca_proj = pca.fit_transform(temp_X)\n",
    "                del(temp_X)\n",
    "\n",
    "                np.random.seed(42)\n",
    "                knn = NearestNeighbors(n_neighbors=neighbors)\n",
    "                knn.fit(pca_proj,Y1)\n",
    "                graph = knn.kneighbors_graph(pca_proj)\n",
    "                knn_feature1 = np.squeeze(np.array(np.sum(graph[:,Y1==1], axis=1) / neighbors)) #sum across rows\n",
    "\n",
    "                if(save):\n",
    "                    np.save(knn_file, knn_feature1)                    \n",
    "\n",
    "\n",
    "            #estimate true faction of doublets \n",
    "            quantile = np.quantile(knn_feature1[true0==2], .25)\n",
    "            num = np.sum(knn_feature1[true0<2]>=quantile)\n",
    "            min_num = int(np.round((sum(Y1==0) *0.05)))\n",
    "            num = np.max([min_num, num])\n",
    "\n",
    "            prob = knn_feature1[Y1==1] / np.sum(knn_feature1[Y1==1])\n",
    "            np.random.seed(seeds[0])\n",
    "            ind = np.random.choice(np.arange(sum(Y1==1)), size=num, p=prob, replace=False)\n",
    "\n",
    "            #ind = sum(Y==0) + ind\n",
    "\n",
    "            #downsample the simulated doublets\n",
    "            enc_ind = np.concatenate([np.arange(sum(Y1==0)), (sum(Y1==0) + ind)])\n",
    "            X1 = X1[enc_ind,:]\n",
    "            Y1 = Y1[enc_ind]\n",
    "            knn_feature1 = knn_feature1[enc_ind]\n",
    "            new_labs1 = labels1[enc_ind]\n",
    "            true1 = true1[enc_ind]\n",
    "            \n",
    "        else:            \n",
    "            #randomly downsample 10% doublets\n",
    "            num = int(np.round((sum(Y1==0) *0.1)))\n",
    "            np.random.seed(seeds[0])\n",
    "            ind = np.random.choice(np.arange(sum(Y1==1)), size=num)\n",
    "\n",
    "            #downsample the simulated doublets\n",
    "            enc_ind = np.concatenate([np.arange(sum(Y1==0)), (sum(Y1==0) + ind)])\n",
    "            X1 = X1[enc_ind,:]\n",
    "            Y1 = Y1[enc_ind]\n",
    "            new_labs1 = labels1[enc_ind]\n",
    "            true1 = true1[enc_ind]\n",
    "            \n",
    "        print('X shape:', X1.shape)\n",
    "        print('Y:', Y1.shape)\n",
    "        print('end downsample')\n",
    "\n",
    "        #re-scale\n",
    "        X1 = np.log2(X1+1)\n",
    "        np.random.seed(42)\n",
    "        scaler = StandardScaler().fit(X1.T)\n",
    "        np.random.seed(42)\n",
    "        X1 = scaler.transform(X1.T).T\n",
    "\n",
    "        #######################################################\n",
    "        ####################### CLUSTER #######################\n",
    "        #######################################################\n",
    "\n",
    "        clust_path = save_path + 'FEAT' + feat + '/' \n",
    "        if not os.path.exists(clust_path):\n",
    "            os.makedirs(clust_path)\n",
    "\n",
    "        clust_file = clust_path + data_name + '_clusters.npy'\n",
    "\n",
    "        if(pl.Path(clust_file).exists() & use_old):\n",
    "            print('read in old clusters')\n",
    "            clust1 = np.load(clust_file)\n",
    "        else:\n",
    "            print('generate new clusters')\n",
    "            if(X1.shape[0]>=1000):\n",
    "                clust1 = fast_cluster(X1, comp=30)\n",
    "            else:\n",
    "                clust1 = cluster(X1, comp=30)\n",
    "\n",
    "            if(save):\n",
    "                np.save(clust_file, clust1)\n",
    "                \n",
    "        print('X shape:', X1.shape)\n",
    "        print('clust shape:', clust1.shape)\n",
    "        print('Y:', Y1.shape)\n",
    "        print('end clust')\n",
    "                \n",
    "        for homo in homos:\n",
    "            print('HOMO: ', homo)\n",
    "            \n",
    "            X2 = X1\n",
    "            Y2 = Y1\n",
    "            true2 = true1\n",
    "            clust2 = clust1\n",
    "            new_labs2 = new_labs1\n",
    "            if(feat=='knnfeat'):\n",
    "                knn_feature2 = knn_feature1\n",
    "            ind_2 = ind\n",
    "            \n",
    "            print('ind2:', ind.shape)\n",
    "            \n",
    "            print('start homo')\n",
    "            print('X2 shape:', X2.shape)\n",
    "            print('Y2:', Y2.shape)\n",
    "            \n",
    "            \n",
    "            if(homo=='remove'):\n",
    "                print('removing homos')\n",
    "                c = clust2[Y2==0]\n",
    "\n",
    "                hetero_ind = c[ind1] != c[ind2]\n",
    "                hetero_ind = hetero_ind[ind_2] #becasue down sampled\n",
    "                hetero_ind = np.concatenate([np.full(sum(Y2==0), True), hetero_ind])\n",
    "\n",
    "                X2 = X2[hetero_ind,:]\n",
    "                Y2 = Y2[hetero_ind]\n",
    "                clust2 = clust2[hetero_ind]\n",
    "                \n",
    "                if(feat=='knnfeat'):\n",
    "                    knn_feature2 = knn_feature2[hetero_ind]\n",
    "                \n",
    "                new_labs2 = new_labs2[hetero_ind]\n",
    "                true2 = true2[hetero_ind]\n",
    "                \n",
    "\n",
    "            print('X shape:', X2.shape)\n",
    "            print('Y:', Y2.shape)\n",
    "            print('end homo')\n",
    "\n",
    "            \n",
    "                \n",
    "            for dim_red in dim_reds:\n",
    "                \n",
    "                X3 = X2\n",
    "                Y3 = Y2\n",
    "                true3 = true2\n",
    "                clust3 = clust2\n",
    "                new_labs3 = new_labs2\n",
    "                if(feat=='knnfeat'):\n",
    "                    knn_feature3 = knn_feature2\n",
    "\n",
    "                print('start enc')\n",
    "                print('X:', X3.shape)\n",
    "                print('Y:', Y3.shape)\n",
    "                \n",
    "                print('DIMRED: ', dim_red)\n",
    "                #######################################################\n",
    "                ######################### VAE #########################\n",
    "                #######################################################\n",
    "                #X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=12345)\n",
    "                \n",
    "                if(dim_red=='clust_vae'):\n",
    "                    print('CLUST VAE')\n",
    "\n",
    "                    vae_path = save_path + 'FEAT' + feat + '/HOMO' + homo + '/'\n",
    "                    if not os.path.exists(vae_path):\n",
    "                        os.makedirs(vae_path)\n",
    "\n",
    "                    vae_file = vae_path + data_name + '_clust_vae_encoding.npy'\n",
    "\n",
    "                    if(pl.Path(vae_file).exists() & use_old):\n",
    "                        print('loading in old VAE encoding')\n",
    "                        encoding = np.load(vae_file)\n",
    "                    else:\n",
    "                        X_train, X_test, clust_train, clust_test = train_test_split(X3, clust3, test_size=0.1, random_state=12345)\n",
    "                        clust_train = tf.one_hot(clust_train, depth=clust3.max()+1)\n",
    "                        clust_test = tf.one_hot(clust_test, depth=clust3.max()+1)\n",
    "\n",
    "                        ngens = X3.shape[1]\n",
    "\n",
    "                        #VAE\n",
    "                        print('generating new VAE encoding')\n",
    "                        tf.random.set_seed(seeds[1])\n",
    "                        vae = define_clust_vae(enc_sze, ngens, clust3.max()+1, LR=LR, clust_weight=clust_weight)\n",
    "\n",
    "                        callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                                    mode = 'min',\n",
    "                                                                    min_delta=0, \n",
    "                                                                    patience=pat, \n",
    "                                                                    verbose=True, \n",
    "                                                                    restore_best_weights=False)\n",
    "\n",
    "                        def scheduler(epoch, lr):\n",
    "                            if epoch < 3:\n",
    "                                return lr\n",
    "                            else:\n",
    "                                return lr * tf.math.exp(-0.75)\n",
    "\n",
    "                        callback2 = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "                        #tf.config.optimizer.set_jit(True)\n",
    "                        hist = vae.fit(x=[X_train],\n",
    "                                       y=[X_train, clust_train],\n",
    "                                       validation_data=([X_test], [X_test, clust_test]),\n",
    "                                       epochs=eps, \n",
    "                                       use_multiprocessing=True,\n",
    "                                       callbacks=[callback, callback2])\n",
    "\n",
    "                        encoder = vae.get_layer('encoder')\n",
    "                        tf.random.set_seed(seeds[2])\n",
    "                        encoding = np.array(tf.convert_to_tensor(encoder(X3)))\n",
    "\n",
    "                        if save:\n",
    "                            np.save(vae_file, encoding)\n",
    "\n",
    "                    del(vae_file)\n",
    "                    print(seeds[1]==29503)\n",
    "                    print(seeds[2]==432809)\n",
    "\n",
    "                if(dim_red=='vae'):\n",
    "                    print('VAE')\n",
    "\n",
    "                    vae_path = save_path + 'FEAT' + feat + '/HOMO' + homo + '/'\n",
    "                    if not os.path.exists(vae_path):\n",
    "                        os.makedirs(vae_path)\n",
    "\n",
    "                    vae_file = vae_path + data_name + '_vae_encoding.npy'\n",
    "\n",
    "                    if(pl.Path(vae_file).exists() & use_old):\n",
    "                        print('loading in old VAE encoding')\n",
    "                        encoding = np.load(vae_file)\n",
    "                    else:\n",
    "                        X_train, X_test, t_train, y_test = train_test_split(X3, Y3, test_size=0.1, random_state=12345)\n",
    "\n",
    "                        ngens = X3.shape[1]\n",
    "\n",
    "                        #VAE\n",
    "                        print('generating new VAE encoding')\n",
    "                        tf.random.set_seed(seeds[1])\n",
    "                        vae = define_vae(enc_sze, ngens)\n",
    "\n",
    "                        callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                                    mode = 'min',\n",
    "                                                                    min_delta=0, \n",
    "                                                                    patience=pat, \n",
    "                                                                    verbose=True, \n",
    "                                                                    restore_best_weights=False)\n",
    "\n",
    "                        def scheduler(epoch, lr):\n",
    "                            if epoch < 3:\n",
    "                                return lr\n",
    "                            else:\n",
    "                                return lr * tf.math.exp(-0.75)\n",
    "\n",
    "                        callback2 = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "                        #tf.config.optimizer.set_jit(True)\n",
    "                        hist = vae.fit(x=X_train,\n",
    "                                       y=X_train,\n",
    "                                       validation_data=(X_test, X_test),\n",
    "                                       epochs=eps, \n",
    "                                       use_multiprocessing=True,\n",
    "                                       callbacks=[callback, callback2])\n",
    "\n",
    "                        encoder = vae.get_layer('encoder')\n",
    "                        tf.random.set_seed(seeds[2])\n",
    "                        encoding = np.array(tf.convert_to_tensor(encoder(X3)))\n",
    "                        \n",
    "                        if save:\n",
    "                            np.save(vae_file, encoding)\n",
    "\n",
    "                    del(vae_file)\n",
    "\n",
    "\n",
    "                if(dim_red=='pca'):\n",
    "\n",
    "                    pca_path = save_path + 'FEAT' + feat + '/HOMO' + homo + '/'\n",
    "                    if not os.path.exists(pca_path):\n",
    "                        os.makedirs(pca_path)\n",
    "\n",
    "                    pca_file = pca_path + data_name + '_pca.npy'\n",
    "\n",
    "                    if(pl.Path(pca_file).exists() & use_old):\n",
    "                        print('loading in old PCA')\n",
    "                        encoding = np.load(pca_file)\n",
    "                    else:\n",
    "                        print('generating new PCA encoding')\n",
    "                        pca = PCA(n_components=enc_sze)\n",
    "                        encoding = pca.fit_transform(X3)\n",
    "                        if(save):\n",
    "                            np.save(pca_file, encoding)\n",
    "\n",
    "                print('X:', X3.shape)\n",
    "                print('Y:', Y3.shape)\n",
    "                #print('enc shape: ', encoding.shape)\n",
    "                #print('knn shape:', knn_feature.shape)\n",
    "                print('end enc')\n",
    "                \n",
    "                if(feat=='knnfeat'):\n",
    "                    encoding = np.vstack([knn_feature3,encoding.T]).T\n",
    "                \n",
    "                #print('enc shape: ', encoding.shape)\n",
    "\n",
    "                print('Y3:', Y3.shape)\n",
    "                #print('encoding:', encoding.shape)\n",
    "                \n",
    "                \n",
    "                U3 = encoding[Y3==0,:]\n",
    "                P3 = encoding[Y3==1,:]\n",
    "\n",
    "                for pu in PUs:\n",
    "                    for clss in clsses:\n",
    "                        \n",
    "                        U4 = U3\n",
    "                        P4 = P3\n",
    "                        true4 = true3\n",
    "                        X4 = X3\n",
    "                        Y4 = Y3\n",
    "                        if (feat=='knnfeat'):\n",
    "                            knn_feature4 = knn_feature3\n",
    "\n",
    "                        print('DIMRED: ', dim_red)\n",
    "                        print('PU: ', pu)\n",
    "                        print('CLSS: ', clss)\n",
    "                        print('FEAT: ', feat)\n",
    "                        print('HOMO: ', homo)\n",
    "\n",
    "                        save_p = save_path + 'FEAT' + feat + '/HOMO' + homo + '/DIMRED' + dim_red + '/PU' + pu + '/CLSS' + clss + '/'\n",
    "                        if not os.path.exists(save_p):\n",
    "                            os.makedirs(save_p)\n",
    "\n",
    "                        save_file = save_p + data_name + '_scores.csv'\n",
    "                        if(pl.Path(save_file).exists() & use_old):\n",
    "                            print('loading in old scores')\n",
    "\n",
    "                            df = pd.read_csv(save_file)\n",
    "\n",
    "                            preds = df.score[Y4==0]\n",
    "                            preds_on_P = df.score[Y4==1]\n",
    "                        else:\n",
    "                            print('generating new scores')\n",
    "\n",
    "                            if(pu=='PU'):\n",
    "                                print('doing PU')\n",
    "                                #######################################################\n",
    "                                ######################### PU ##########################\n",
    "                                #######################################################\n",
    "\n",
    "                                num_cells = P4.shape[0]*k_mult#1000\n",
    "                                k = int(U4.shape[0] / num_cells)\n",
    "                                if(k<2):\n",
    "                                    k=2\n",
    "\n",
    "                                if(clss=='NN'):\n",
    "                                    hist = epoch_PU2(U4, P4, k, N, 250, seeds=seeds[3:], puLR=1e-3)\n",
    "            \n",
    "                                    y=np.log(hist.history['loss'])\n",
    "                                    x=np.arange(len(y))\n",
    "                                    yhat = savgol_filter(y, 7, 1) \n",
    "\n",
    "                                    y=yhat\n",
    "                                    x=np.arange(len(y))\n",
    "\n",
    "                                    kneedle = KneeLocator(x, y, S=10, curve='convex', direction='decreasing')\n",
    "\n",
    "                                    knee = kneedle.knee\n",
    "\n",
    "                                    if knee==None:\n",
    "                                        knee = 250\n",
    "                                    elif(num < 500):#add epochs if ther aren't enough cells\n",
    "                                        print('added 100')\n",
    "                                        knee = knee+100\n",
    "                                    elif knee<20:\n",
    "                                        knee = 20\n",
    "                                    elif knee>250:\n",
    "                                        knee = 250\n",
    "\n",
    "                                    print('KNEE:', knee)   \n",
    "\n",
    "                                else:\n",
    "                                    knee=250\n",
    "\n",
    "                                preds, preds_on_P, hists, _, _, _ = PU(U4, P4, k, N, knee, clss=clss, seeds=seeds[3:], puLR=puLR)\n",
    "\n",
    "                            if(pu=='noPU'):\n",
    "                                print('NO PU')\n",
    "\n",
    "                                preds, preds_on_P = noPU(U4, P4, cls_eps, clss=clss, seeds=seeds[3:], puPat=5, puLR=1e-3, num_layers=1)\n",
    "\n",
    "\n",
    "                        #RESULTS\n",
    "                        preds_sing = preds[true4[true4<2]==0]\n",
    "                        preds_doub_test = preds[true4[true4<2]==1]\n",
    "                        preds_doub_train = preds_on_P\n",
    "                        labs = ['singlet', 'actual doublet', 'simulated doublet']\n",
    "                        cols = np.concatenate([preds, preds_on_P])\n",
    "\n",
    "                        #SAVE SCORES\n",
    "                        #new_labs = labels[enc_ind]\n",
    "                        tmp1 = np.zeros((len(new_labs3), 2))\n",
    "                        df = pd.DataFrame(tmp1, index=new_labs3, columns=['annotation', 'score'])\n",
    "                        df.annotation = new_labs3\n",
    "                        df.score = np.concatenate([preds, preds_on_P])\n",
    "                        if(save):\n",
    "                            df.to_csv(save_file) \n",
    "\n",
    "                        #PR and ROC curves\n",
    "                        plt.figure(4)\n",
    "                        res = get_dbl_metrics(true[true<2], preds)\n",
    "                        plt.show()\n",
    "                        plt.close()\n",
    "\n",
    "                        #save AUCs\n",
    "                        hm_pr = pd.DataFrame(np.array(res).T, index=['AUROC', 'AUPRC', 'AP']).T\n",
    "                        if(save):\n",
    "                            hm_pr.to_csv(save_p + data_name + '_scores_ROC_PR_area_ALL.csv') \n",
    "\n",
    "                        del(preds)\n",
    "                        del(preds_on_P)\n",
    "\n",
    "                del(U4)\n",
    "                del(P4)\n",
    "\n",
    "            del(encoding)\n",
    "                \n",
    "        if feat=='knnfeat':\n",
    "            del(knn_feature1) \n",
    "            del(knn_feature2) \n",
    "            del(knn_feature3) \n",
    "            del(knn_feature4) \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e5dfb6-d3e0-4026-80b3-357590012963",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''clusters_real = np.load('../results_PU/final_vaeda_result/hyper_PcaComp30_ClustWeight20000/J293t-dm_clusters_real.npy')\n",
    "clusters_sim = np.load('../results_PU/final_vaeda_result/hyper_PcaComp30_ClustWeight20000/J293t-dm_clusters_sim.npy')\n",
    "emb_real = np.load('../results_PU/final_vaeda_result/hyper_PcaComp30_ClustWeight20000/J293t-dm_embedding_real.npy')\n",
    "emb_sim = np.load('../results_PU/final_vaeda_result/hyper_PcaComp30_ClustWeight20000/J293t-dm_embedding_sim.npy')\n",
    "df_scores = pd.read_csv('../results_PU/final_vaeda_result/hyper_PcaComp30_ClustWeight20000/J293t-dm_scores_ROC_PR_area_ALL.csv')\n",
    "knn_feat_real = np.load('../results_PU/final_vaeda_result/hyper_PcaComp30_ClustWeight20000/J293t-dm_knn_feature_real.npy')\n",
    "knn_feat_sim = np.load('../results_PU/final_vaeda_result/hyper_PcaComp30_ClustWeight20000/J293t-dm_knn_feature_sim.npy')\n",
    "sim_scores = np.load('../results_PU/final_vaeda_result/hyper_PcaComp30_ClustWeight20000/J293t-dm_scores_on_sim.npy')\n",
    "scores = np.load('../results_PU/final_vaeda_result/hyper_PcaComp30_ClustWeight20000/J293t-dm_scores.npy')\n",
    "sim_ind = np.load('../results_PU/final_vaeda_result/hyper_PcaComp30_ClustWeight20000/J293t-dm_sim_ind.npy')\n",
    "time = pd.read_csv('../results_PU/final_vaeda_result/hyper_PcaComp30_ClustWeight20000/J293t-dm_time.csv')\n",
    "dat_sim = scs.load_npz('../results_PU/final_vaeda_result/hyper_PcaComp30_ClustWeight20000/J293t-dm_sim_doubs.npz')\n",
    "sim_which = np.load('../results_PU/final_vaeda_result/hyper_PcaComp30_ClustWeight20000/J293t-dm_which_sim_doubs.npy')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5630c617-65c4-451e-94ae-ad64ff4d7a18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37165065-caa4-432e-8129-43f003989d33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv",
   "language": "python",
   "name": "newenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
