[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook CLUST-VAEDA.ipynb to notebook
2021-11-02 21:20:36.590083: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2021-11-02 21:20:36.590120: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2021-11-02 21:20:41.435185: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-11-02 21:20:41.435497: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2021-11-02 21:20:41.435526: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)
2021-11-02 21:20:41.435573: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (c049b509c347): /proc/driver/nvidia/version does not exist
2021-11-02 21:20:41.438652: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-11-02 21:20:41.872106: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
2021-11-02 21:20:41.921116: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-11-02 21:20:41.921653: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3199795000 Hz
[NbConvertApp] Writing 144221 bytes to CLUST-VAEDA.nbconvert.ipynb
[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook CLUST-VAEDA.ipynb to notebook
2021-11-02 21:35:10.045874: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2021-11-02 21:35:10.045908: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2021-11-02 21:35:44.863831: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-11-02 21:35:44.864145: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2021-11-02 21:35:44.864163: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)
2021-11-02 21:35:44.864206: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (c049b509c347): /proc/driver/nvidia/version does not exist
2021-11-02 21:35:44.866996: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-11-02 21:35:45.260951: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
2021-11-02 21:35:45.500373: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-11-02 21:35:45.500874: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3199795000 Hz
[NbConvertApp] Writing 2141513 bytes to CLUST-VAEDA.nbconvert.ipynb
[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.
[NbConvertApp] Converting notebook CLUST-VAEDA-ablation.ipynb to notebook
2021-11-30 22:30:57.625991: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2021-11-30 22:30:57.626030: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Traceback (most recent call last):
  File "/home/hannah/miniconda3/envs/newenv/bin/jupyter-nbconvert", line 11, in <module>
    sys.exit(main())
  File "/home/hannah/miniconda3/envs/newenv/lib/python3.8/site-packages/jupyter_core/application.py", line 254, in launch_instance
    return super(JupyterApp, cls).launch_instance(argv=argv, **kwargs)
  File "/home/hannah/miniconda3/envs/newenv/lib/python3.8/site-packages/traitlets/config/application.py", line 846, in launch_instance
    app.start()
  File "/home/hannah/miniconda3/envs/newenv/lib/python3.8/site-packages/nbconvert/nbconvertapp.py", line 346, in start
    self.convert_notebooks()
  File "/home/hannah/miniconda3/envs/newenv/lib/python3.8/site-packages/nbconvert/nbconvertapp.py", line 518, in convert_notebooks
    self.convert_single_notebook(notebook_filename)
  File "/home/hannah/miniconda3/envs/newenv/lib/python3.8/site-packages/nbconvert/nbconvertapp.py", line 483, in convert_single_notebook
    output, resources = self.export_single_notebook(notebook_filename, resources, input_buffer=input_buffer)
  File "/home/hannah/miniconda3/envs/newenv/lib/python3.8/site-packages/nbconvert/nbconvertapp.py", line 412, in export_single_notebook
    output, resources = self.exporter.from_filename(notebook_filename, resources=resources)
  File "/home/hannah/miniconda3/envs/newenv/lib/python3.8/site-packages/nbconvert/exporters/exporter.py", line 181, in from_filename
    return self.from_file(f, resources=resources, **kw)
  File "/home/hannah/miniconda3/envs/newenv/lib/python3.8/site-packages/nbconvert/exporters/exporter.py", line 199, in from_file
    return self.from_notebook_node(nbformat.read(file_stream, as_version=4), resources=resources, **kw)
  File "/home/hannah/miniconda3/envs/newenv/lib/python3.8/site-packages/nbconvert/exporters/notebook.py", line 32, in from_notebook_node
    nb_copy, resources = super().from_notebook_node(nb, resources, **kw)
  File "/home/hannah/miniconda3/envs/newenv/lib/python3.8/site-packages/nbconvert/exporters/exporter.py", line 143, in from_notebook_node
    nb_copy, resources = self._preprocess(nb_copy, resources)
  File "/home/hannah/miniconda3/envs/newenv/lib/python3.8/site-packages/nbconvert/exporters/exporter.py", line 318, in _preprocess
    nbc, resc = preprocessor(nbc, resc)
  File "/home/hannah/miniconda3/envs/newenv/lib/python3.8/site-packages/nbconvert/preprocessors/base.py", line 47, in __call__
    return self.preprocess(nb, resources)
  File "/home/hannah/miniconda3/envs/newenv/lib/python3.8/site-packages/nbconvert/preprocessors/execute.py", line 84, in preprocess
    self.preprocess_cell(cell, resources, index)
  File "/home/hannah/miniconda3/envs/newenv/lib/python3.8/site-packages/nbconvert/preprocessors/execute.py", line 105, in preprocess_cell
    cell = self.execute_cell(cell, index, store_history=True)
  File "/home/hannah/miniconda3/envs/newenv/lib/python3.8/site-packages/nbclient/util.py", line 74, in wrapped
    return just_run(coro(*args, **kwargs))
  File "/home/hannah/miniconda3/envs/newenv/lib/python3.8/site-packages/nbclient/util.py", line 53, in just_run
    return loop.run_until_complete(coro)
  File "/home/hannah/miniconda3/envs/newenv/lib/python3.8/asyncio/base_events.py", line 616, in run_until_complete
    return future.result()
  File "/home/hannah/miniconda3/envs/newenv/lib/python3.8/site-packages/nbclient/client.py", line 857, in async_execute_cell
    self._check_raise_for_error(cell, exec_reply)
  File "/home/hannah/miniconda3/envs/newenv/lib/python3.8/site-packages/nbclient/client.py", line 760, in _check_raise_for_error
    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
------------------
for file in files:

    data_name = file[:-19]
    print(data_name) 
    
    save_path = '../results_PU/ablation_analysis/'
    if not os.path.exists(save_path):
        os.makedirs(save_path)
    print(save_path)
    
    #scvae-dbl-btch/data/sce_normalized_data_inflate/pdx-MULTI_real_logcounts.mtx
    ano_path  = '../data/'+ data_dir + '/' + data_name + '_anno.csv'
    vae_path = save_path + data_name + '_vae_embedding.npy'
    
    
    #scvae-dbl-btch/data/sce_normalized_data_inflate/pdx-MULTI_real_logcounts.mtx
    npz_real_path = '../data/npz_objs/' + data_name + '_real_counts.npz'
    npz_sim_path  = '../data/npz_objs/' + data_name + '_sim_counts.npz'
    sim_ind_path  = '../data/npz_objs/' + data_name + '_sim_ind.npy'

    ano_path  = '../data/'+ data_dir + '/' + data_name + '_anno.csv'

    #- READ IN COUNTS
    npz_real = pl.Path(npz_real_path)
    npz_sim  = pl.Path(npz_sim_path)

    if (npz_real.exists()):
        print('loading in real npz')
        dat_real = scs.load_npz(npz_real)
    else:
        print('does not exist')
        quit()

    Xr = scs.csr_matrix(dat_real).toarray()

    if (npz_sim.exists() & use_old):
        print('loading in sim npz')
        dat_sim = scs.load_npz(npz_sim)
        Xs = scs.csr_matrix(dat_sim).toarray()
    else:
        print('generating new sim npz')

        Xs, ind1, ind2 = sim_inflate(Xr)
    
    dat = np.vstack([Xr,Xs])
    
    #- READ IN BARCODE ANNOTATIONS
    ano = pd.read_csv(ano_path)
    true = pd.factorize(ano.x)[0]
    labels = ano.x
    if (labels[0]=='doublet'):
        tmp = true + 3
        tmp[tmp==3] = 1
        tmp[tmp==4] = 0
        true = tmp
    true = np.concatenate([true, np.full(Xs.shape[0],2)])
    labels = np.concatenate([labels, np.full(Xs.shape[0],'simulated')])

    
    #- HVGs - NEED TO DO SOMETHING ELSE!    
    var = np.var(dat, axis=0)
    np.random.seed(3900362577)
    hvgs = np.argpartition(var, -2000)[-2000:]      
    

    #######################################################
    ######################### KNN #########################
    #######################################################
    
    TRUE = true
    
    for feat in feats:
        
        save_path = '../results_PU/ablation_analysis/'
        
        X = dat[:,hvgs]
        Y = (TRUE>1).astype(int)
    
        if feat == 'knnfeat':
            
            knn_file = save_path + data_name + '_knn_feature.npy'
            
            if(pl.Path(knn_file).exists() & use_old):
                print('loading old knn')
                knn_feature = np.load(knn_file)
            
            else:
                print('generating new knn')
                #HYPERPARAMS
                neighbors = int(np.sqrt(X.shape[0]))
                comp=20

                #SCALING
                temp_X = np.log2(X+1)
                np.random.seed(42)
                scaler = StandardScaler().fit(temp_X.T)
                np.random.seed(42)
                temp_X = scaler.transform(temp_X.T).T

                #KNN
                np.random.seed(42)
                pca = PCA(n_components=comp)
                pca_proj = pca.fit_transform(temp_X)
                del(temp_X)

                np.random.seed(42)
                knn = NearestNeighbors(n_neighbors=neighbors)
                knn.fit(pca_proj,Y)
                graph = knn.kneighbors_graph(pca_proj)
                knn_feature = np.squeeze(np.array(np.sum(graph[:,Y==1], axis=1) / neighbors)) #sum across rows

                if(save):
                    np.save(knn_file, knn_feature)                    


            #estimate true faction of doublets 
            quantile = np.quantile(knn_feature[TRUE==2], .25)
            num = np.sum(knn_feature[TRUE<2]>=quantile)
            min_num = int(np.round((sum(Y==0) *0.05)))
            num = np.max([min_num, num])

            prob = knn_feature[Y==1] / np.sum(knn_feature[Y==1])
            np.random.seed(seeds[0])
            ind = np.random.choice(np.arange(sum(Y==1)), size=num, p=prob, replace=False)

            #ind = sum(Y==0) + ind

            #downsample the simulated doublets
            enc_ind = np.concatenate([np.arange(sum(Y==0)), (sum(Y==0) + ind)])
            X = X[enc_ind,:]
            Y = Y[enc_ind]
            knn_feature = knn_feature[enc_ind]
            new_labs = labels[enc_ind]
            true = TRUE[enc_ind]

        else:
            #randomly downsample 10% doublets
            num = int(np.round((sum(Y==0) *0.1)))
            ind = np.random.choice(np.arange(sum(Y==1)), size=num)

            #downsample the simulated doublets
            enc_ind = np.concatenate([np.arange(sum(Y==0)), (sum(Y==0) + ind)])
            X = dat[enc_ind,:]
            X = X[:,hvgs]
            Y = Y[enc_ind]
            new_labs = labels[enc_ind]
            true = TRUE[enc_ind]

        #re-scale
        X = np.log2(X+1)
        np.random.seed(42)
        scaler = StandardScaler().fit(X.T)
        np.random.seed(42)
        X = scaler.transform(X.T).T

        #######################################################
        ####################### CLUSTER #######################
        #######################################################

        clust_path = save_path + 'FEAT' + feat + '/' 
        if not os.path.exists(clust_path):
            os.makedirs(clust_path)

        clust_file = clust_path + data_name + '_clusters.npy'

        if(pl.Path(clust_file).exists() & use_old):
            print('read in old clusters')
            clust = np.load(clust_file)
        else:
            print('generate new clusters')
            if(X.shape[0]>=1000):
                clust = fast_cluster(X, comp=20)
            else:
                clust = cluster(X, comp=20)

            if(save):
                np.save(clust_file, clust)
                

        X_keep = X
        Y_keep = Y
        true_keep = true
        clust_keep = clust
        new_labs_keep = new_labs
        if(feat=='knnfeat'):
            knn_feature_keep = knn_feature
        
        print('ind1:', ind.shape)
        
        for homo in homos:
            print('HOMO: ', homo)
            
            print('ind2:', ind.shape)
            
            if(homo=='remove'):
                #ind1 = ind1[ind]
                #ind2 = ind2[ind]
                c = clust[Y==0]

                hetero_ind = c[ind1] != c[ind2]
                hetero_ind = hetero_ind[ind] #becasue down sampled
                hetero_ind = np.concatenate([np.full(sum(Y==0), True), hetero_ind])

                X = X_keep[hetero_ind,:]
                Y = Y_keep[hetero_ind]
                clust = clust_keep[hetero_ind]
                
                if(feat=='knnfeat'):
                    knn_feature = knn_feature_keep[hetero_ind]
                
                new_labs = new_labs_keep[hetero_ind]
                true = true_keep[hetero_ind]
                
            else:
                X = X_keep
                Y = Y_keep
                true = true_keep
                clust = clust_keep
                new_labs = new_labs_keep
                if(feat=='knnfeat'):
                    knn_feature = knn_feature_keep

            print('Y2:', Y.shape)
                
            for dim_red in dim_reds:

                print('X:', X.shape)
                
                print('DIMRED: ', dim_red)
                #######################################################
                ######################### VAE #########################
                #######################################################
                #X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=12345)

                if(dim_red=='clust_vae'):
                    print('CLUST VAE')

                    vae_path = save_path + 'FEAT' + feat + '/HOMO' + homo + '/'
                    if not os.path.exists(vae_path):
                        os.makedirs(vae_path)

                    vae_file = vae_path + data_name + '_clust_vae_encoding.npy'

                    if(pl.Path(vae_file).exists() & use_old):
                        print('loading in old VAE encoding')
                        encoding = np.load(vae_file)
                    else:
                        X_train, X_test, clust_train, clust_test = train_test_split(X, clust, test_size=0.1, random_state=12345)
                        clust_train = tf.one_hot(clust_train, depth=clust.max()+1)
                        clust_test = tf.one_hot(clust_test, depth=clust.max()+1)

                        ngens = X.shape[1]

                        #VAE
                        print('generating new VAE encoding')
                        tf.random.set_seed(seeds[1])
                        vae = define_clust_vae(enc_sze, ngens, clust.max()+1, LR=LR)

                        callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss',
                                                                    mode = 'min',
                                                                    min_delta=0, 
                                                                    patience=pat, 
                                                                    verbose=True, 
                                                                    restore_best_weights=False)

                        def scheduler(epoch, lr):
                            if epoch < 3:
                                return lr
                            else:
                                return lr * tf.math.exp(-0.75)

                        callback2 = tf.keras.callbacks.LearningRateScheduler(scheduler)

                        #tf.config.optimizer.set_jit(True)
                        hist = vae.fit(x=[X_train],
                                       y=[X_train, clust_train],
                                       validation_data=([X_test], [X_test, clust_test]),
                                       epochs=eps, 
                                       use_multiprocessing=True,
                                       callbacks=[callback, callback2])

                        encoder = vae.get_layer('encoder')
                        tf.random.set_seed(seeds[2])
                        encoding = np.array(tf.convert_to_tensor(encoder(X)))

                        if save:
                            np.save(vae_file, encoding)

                    del(vae_file)

                if(dim_red=='vae'):
                    print('VAE')

                    vae_path = save_path + 'FEAT' + feat + '/HOMO' + homo + '/'
                    if not os.path.exists(vae_path):
                        os.makedirs(vae_path)

                    vae_file = vae_path + data_name + '_vae_encoding.npy'

                    if(pl.Path(vae_file).exists() & use_old):
                        print('loading in old VAE encoding')
                        encoding = np.load(vae_file)
                    else:
                        X_train, X_test, t_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=12345)

                        ngens = X.shape[1]

                        #VAE
                        print('generating new VAE encoding')
                        tf.random.set_seed(seeds[1])
                        vae = define_vae(enc_sze, ngens)

                        callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss',
                                                                    mode = 'min',
                                                                    min_delta=0, 
                                                                    patience=pat, 
                                                                    verbose=True, 
                                                                    restore_best_weights=False)

                        def scheduler(epoch, lr):
                            if epoch < 3:
                                return lr
                            else:
                                return lr * tf.math.exp(-0.75)

                        callback2 = tf.keras.callbacks.LearningRateScheduler(scheduler)

                        #tf.config.optimizer.set_jit(True)
                        hist = vae.fit(x=X_train,
                                       y=X_train,
                                       validation_data=(X_test, X_test),
                                       epochs=eps, 
                                       use_multiprocessing=True,
                                       callbacks=[callback, callback2])

                        encoder = vae.get_layer('encoder')
                        tf.random.set_seed(seeds[2])
                        encoding = np.array(tf.convert_to_tensor(encoder(X)))
                        
                        if save:
                            np.save(vae_file, encoding)

                    del(vae_file)


                if(dim_red=='pca'):

                    pca_path = save_path + 'FEAT' + feat + '/HOMO' + homo + '/'
                    if not os.path.exists(pca_path):
                        os.makedirs(pca_path)

                    pca_file = pca_path + data_name + '_pca.npy'

                    if(pl.Path(pca_file).exists() & use_old):
                        print('loading in old PCA')
                        encoding = np.load(pca_file)
                    else:
                        print('generating new PCA encoding')
                        pca = PCA(n_components=enc_sze)
                        encoding = pca.fit_transform(X)
                        if(save):
                            np.save(pca_file, encoding)

                print('enc shape: ', encoding.shape)
                
                if(feat=='knnfeat'):
                    encoding = np.vstack([knn_feature,encoding.T]).T
                
                print('enc shape: ', encoding.shape)

                print('Y3:', Y.shape)
                print('encoding:', encoding.shape)
                
                U = encoding[Y==0,:]
                P = encoding[Y==1,:]

                for pu in PUs:
                    for clss in clsses:

                        print('DIMRED: ', dim_red)
                        print('PU: ', pu)
                        print('CLSS: ', clss)
                        print('FEAT: ', feat)
                        print('HOMO: ', homo)

                        save_p = save_path + 'FEAT' + feat + '/HOMO' + homo + '/DIMRED' + dim_red + '/PU' + pu + '/CLSS' + clss + '/'
                        if not os.path.exists(save_p):
                            os.makedirs(save_p)

                        save_file = save_p + data_name + '_scores.csv'
                        if(pl.Path(save_file).exists() & use_old):
                            print('loading in old scores')

                            df = pd.read_csv(save_file)

                            preds = df.score[Y==0]
                            preds_on_P = df.score[Y==1]
                        else:
                            print('generating new scores')

                            if(pu=='PU'):
                                print('doing PU')
                                #######################################################
                                ######################### PU ##########################
                                #######################################################

                                num_cells = P.shape[0]*k_mult#1000
                                k = int(U.shape[0] / num_cells)
                                if(k<2):
                                    k=2

                                if(clss=='NN'):
                                    hist = epoch_PU2(U, P, k, N, 250, seeds=seeds[3:], puLR=1e-3)
            
                                    y=np.log(hist.history['loss'])
                                    x=np.arange(len(y))
                                    yhat = savgol_filter(y, 7, 1) 

                                    y=yhat
                                    x=np.arange(len(y))

                                    kneedle = KneeLocator(x, y, S=10, curve='convex', direction='decreasing')

                                    knee = kneedle.knee

                                    if(num < 500):#add epochs if ther aren't enough cells
                                        print('added 100')
                                        knee = knee+100

                                    if knee==None:
                                        knee = 250

                                    if knee<20:
                                        knee = 20

                                    if knee>250:
                                        knee = 250

                                    print('KNEE:', knee)   

                                else:
                                    knee=250

                                preds, preds_on_P, hists, val_hists, auc_hists, val_auc = PU(U, P, k, N, knee, clss=clss, seeds=seeds[3:], puLR=puLR, num_layers=pu_num_layers, stop_metric=stop_metric)

                            if(pu=='noPU'):
                                print('NO PU')

                                preds, preds_on_P = noPU(U, P, cls_eps, clss=clss, seeds=seeds[3:], puPat=5, puLR=1e-3, num_layers=1)


                        #RESULTS
                        preds_sing = preds[true[true<2]==0]
                        preds_doub_test = preds[true[true<2]==1]
                        preds_doub_train = preds_on_P
                        labs = ['singlet', 'actual doublet', 'simulated doublet']
                        cols = np.concatenate([preds, preds_on_P])

                        #SAVE SCORES
                        #new_labs = labels[enc_ind]
                        tmp1 = np.zeros((len(new_labs), 2))
                        df = pd.DataFrame(tmp1, index=new_labs, columns=['annotation', 'score'])
                        df.annotation = new_labs
                        df.score = np.concatenate([preds, preds_on_P])
                        if(save):
                            df.to_csv(save_file) 

                        #PR and ROC curves
                        plt.figure(4)
                        res = get_dbl_metrics(true[true<2], preds)
                        plt.show()
                        plt.close()

                        #save AUCs
                        hm_pr = pd.DataFrame(np.array(res).T, index=['AUROC', 'AUPRC', 'AP']).T
                        if(save):
                            hm_pr.to_csv(save_p + data_name + '_scores_ROC_PR_area_ALL.csv') 

                        del(preds)
                        del(preds_on_P)

                del(U)
                del(P)
                del(encoding)

        if feat=='knnfeat':
            del(knn_feature) 




------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mNameError[0m                                 Traceback (most recent call last)
[0;32m/tmp/ipykernel_32742/423160299.py[0m in [0;36m<module>[0;34m[0m
[1;32m    198[0m                 [0mc[0m [0;34m=[0m [0mclust[0m[0;34m[[0m[0mY[0m[0;34m==[0m[0;36m0[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[1;32m    199[0m [0;34m[0m[0m
[0;32m--> 200[0;31m                 [0mhetero_ind[0m [0;34m=[0m [0mc[0m[0;34m[[0m[0mind1[0m[0;34m][0m [0;34m!=[0m [0mc[0m[0;34m[[0m[0mind2[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    201[0m                 [0mhetero_ind[0m [0;34m=[0m [0mhetero_ind[0m[0;34m[[0m[0mind[0m[0;34m][0m [0;31m#becasue down sampled[0m[0;34m[0m[0;34m[0m[0m
[1;32m    202[0m                 [0mhetero_ind[0m [0;34m=[0m [0mnp[0m[0;34m.[0m[0mconcatenate[0m[0;34m([0m[0;34m[[0m[0mnp[0m[0;34m.[0m[0mfull[0m[0;34m([0m[0msum[0m[0;34m([0m[0mY[0m[0;34m==[0m[0;36m0[0m[0;34m)[0m[0;34m,[0m [0;32mTrue[0m[0;34m)[0m[0;34m,[0m [0mhetero_ind[0m[0;34m][0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;31mNameError[0m: name 'ind1' is not defined
NameError: name 'ind1' is not defined

